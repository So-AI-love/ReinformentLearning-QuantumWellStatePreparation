{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jyj/anaconda3/envs/PythonData/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from qutip import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQL optimization of asymmetric Szilard engine\n",
    "This program uses Deep Q-Learning, to find protocols which split the quantum wavefunction of an asymmetric Szilard engine in half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Szilard_env:\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.max_lvl = 4\n",
    "        \n",
    "        self.viewer = None\n",
    "        self.action_space = np.array([-1024,-512,-256,-128,-64,-32,-16,-8,-4,-2,0\n",
    "                                      ,2,4,8,16,32,64,128,256,512,1024])*10\n",
    "        self.action_space = np.reshape(self.action_space, (-1,1))\n",
    "        \n",
    "        self.state_space = np.array([0,0])\n",
    "        self.state_space = np.reshape(self.state_space, (-1,1))\n",
    "\n",
    "        self.max_alfa = 3000.0       #这是啥？\n",
    "        self.t_max = 5                 #最大时间\n",
    "        self.N_alfa = 50               #多少个alpha值\n",
    "        self.alfa_dt = self.t_max/self.N_alfa    #时间步长\n",
    "        \n",
    "        self.sigma1 = 0.05    # if smaller than sigma2 gives more punishment for excitations  这哥两是啥？\n",
    "        self.sigma2 = 0.05\n",
    "        \n",
    "        self.best_protocol = [None]*self.N_alfa        #最佳解  （alpha作为t的函数，注意t是离散的）\n",
    "        self.protocol = [None]*self.N_alfa             #一般解\n",
    "        self.best_reward = 0                            #best_reward和score的关系是啥 \n",
    "        self.scores = []\n",
    "        \n",
    "        self.interp_energies = []      \n",
    "        self.interp_overlaps = []\n",
    "        k = 0\n",
    "        \n",
    "        self.N_asymmetries = 6\n",
    "        self.asymmetries = []\n",
    "        \n",
    "        for n in range (self.N_asymmetries):\n",
    "            asymmetry = round(0.53 + 0.002*n,3)\n",
    "            self.asymmetries.append(asymmetry)                     #考虑哪些个不对称性的case\n",
    "            energy, overlap = self.interp_generator(asymmetry)     #d对每个不对称case, 生成能级和交叠作为时间的差值函数（generator形式）\n",
    "            self.interp_energies.append(energy)      \n",
    "            self.interp_overlaps.append(overlap)\n",
    "        \n",
    "        ## Here we can change the reward distribution to favour certain asymmetries\n",
    "        \n",
    "        #r_min  = 0.50\n",
    "        #N_0 = -self.N_asymmetries/np.log(r_min)\n",
    "        #x = np.array([n for n in range (self.N_asymmetries)])\n",
    "        #self.reward_distribution = np.exp(-x/N_0)\n",
    "        self.reward_distribution = np.ones(self.N_asymmetries)\n",
    "            \n",
    "    ## Interpolate data files to calculate eigenstates of the single-particle-box.  \n",
    "    \n",
    "    def interp_generator(self,a):\n",
    "        #给定不对称位置，load max_lvl 个能量和波函数叠加作为时间函数的数据，再生成差值函数\n",
    "        \n",
    "        max_lvl = 4 \n",
    "\n",
    "        tab_alfa = np.loadtxt('/home/vegard/Split-wavefunction/data/alfa/alfa_a_' + \"{:.3f}\".format(a) + '.txt', delimiter = ',')        \n",
    "        energies = np.loadtxt('/home/vegard/Split-wavefunction/data/energies/Energies_a_' + \"{:.3f}\".format(a) + '.txt', delimiter = ',')\n",
    "        overlap = np.loadtxt('/home/vegard/Split-wavefunction/data/overlap/overlap_a_' + \"{:.3f}\".format(a)+ '.txt', delimiter = ',')\n",
    "\n",
    "        overlap = np.reshape(overlap, [max_lvl,max_lvl,5001], order = 'F')  #为什么5001？？？？？？?\n",
    "\n",
    "        interp_energy = [None]*max_lvl\n",
    "\n",
    "        for n in range(max_lvl):\n",
    "            f = interp1d(tab_alfa,energies[:,n], kind = 'cubic')   #给定n‘th level,有一系列的能量对应一系列的alpha值（不同时刻）\n",
    "            interp_energy[n] = f  #这里的f是一个函数“eturns a function whose call method uses interpolation to find the value of new points.”\n",
    "\n",
    "        interp_overlap = [[0] * max_lvl for i in range(max_lvl)] #给定n'th level,它和另外的每一个level之间都有overlap\n",
    "\n",
    "        for n in range(max_lvl):\n",
    "            for m in range(max_lvl):\n",
    "                interp_overlap[n][m] = interp1d(tab_alfa,overlap[n,m,:], kind = 'cubic') #对应每两个level,对每一个时刻（或alpha的值）都有overlap的值\n",
    "\n",
    "        return interp_energy, interp_overlap\n",
    "    \n",
    "    ## Interpolate the discrete protocol obtained from the agent at the end of each episode, and calculate the reward.\n",
    "    def interp_reward(self, N_t):\n",
    "\n",
    "        time_interp = np.array([ n*self.alfa_dt for n in range (self.N_alfa+1) ]) #线性时间\n",
    "        \n",
    "        alfa_interp = interp1d( time_interp,self.protocol, kind = 'cubic')  #protocol作为时间的函数进行差值\n",
    "      \n",
    "        time = np.linspace(0,self.t_max,N_t)       #这里N_t可以大大于N_alfa吧   \n",
    "        alfa = alfa_interp(time)                     #\n",
    "        alfa = np.clip(alfa, 0, 5000)   #限制alpha在[0,5000]之间, 对应函数interaction_int里面的alfa_in\n",
    "    \n",
    "        dt = time[1]-time[0]  \n",
    "        max_lvl = self.max_lvl\n",
    " \n",
    "        E = []\n",
    "        for t in range(N_t):\n",
    "            E.append([self.interp_energy[n](alfa[t]) for n in range(max_lvl)]) #self.interp_energy 是给定asymmetry的差值函数\n",
    "        E = np.array(E)\n",
    "        \n",
    "        phi = []\n",
    "        for t in range(N_t):\n",
    "                integral = [np.trapz(x=time[0:t+1], y= E[0:t+1,n]) for n in range(max_lvl)]\n",
    "                phi.append(integral)  #对应每个时刻，都可以求出相位表达式，即文2的appendix: A(4)\n",
    "        phi = np.array(phi)   \n",
    "\n",
    "        def interaction_int(n, alfa_in, f = []):\n",
    "            test = sum([self.interp_overlap[n][m](abs(alfa_in))*np.exp(1j*(phi[t,n] - phi[t,m]) )*f[m] for m in range(max_lvl) if m != n ])*dalfa\n",
    "            return test  #根据我的推导，这里的interp_overlap[n][m](abs(alfa_in))=<psi(n)|dH/dt|psi(m)>/(E_n-E_m)\n",
    "        ##问题是，作为函数变量的alfa_in怎么能提供dH/dt里面的alpha对时间求导项？最后乘的dalfa？\n",
    "\n",
    "        C = np.array([ 0 for n in range(max_lvl) ])\n",
    "        C[0] = 1\n",
    "        C_new = np.array( [ 0 for n in range(max_lvl) ])\n",
    "        C_t = []\n",
    "        C_t.append(C)\n",
    "\n",
    "        for t in range(N_t -1):\n",
    "            \n",
    "            dalfa = (alfa[t+1] - alfa[t])/dt   #alpha的导数\n",
    "\n",
    "            k1 = np.array([dt*interaction_int(n, alfa[t], C ) for n in range(max_lvl)])\n",
    "            k2 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt/2, C + k1/2 ) for n in range(max_lvl)])\n",
    "            k3 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt/2, C + k2/2 ) for n in range(max_lvl)])\n",
    "            k4 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt, C + k3) for n in range(max_lvl)])\n",
    "\n",
    "            C_new = C + (k1 + 2*k2 + 2*k3 + k4)/6   #这是某种迭代方法\n",
    "\n",
    "            C = C_new\n",
    "\n",
    "            C_t.append(C)#有必要把每个时刻的波函数都保存下来吗？\n",
    "\n",
    "        c1 = abs(C[0])**2\n",
    "        c2 = abs(C[1])**2\n",
    "        \n",
    "        reward = np.exp( -((c1 + c2 - 1))**2/self.sigma1 - ((c1 - c2))**2/self.sigma2 )#嗯，这两个要求都需要成立！\n",
    "        \n",
    "\n",
    "        #leakage = sum( abs(C[n])**2 for n in range(2,max_lvl) )\n",
    "        \n",
    "        #print(\"Normalization_int: {}\" .format(sum(abs(C)**2)))\n",
    "        \n",
    "        return reward   \n",
    "\n",
    "    ## Reset initial state after every episode     \n",
    "    def reset(self):\n",
    "\n",
    "        self.r = random.randrange(self.N_asymmetries)            #随便找一个偏离对称的几何结构，即epsilon,或者,a,b\n",
    "        self.interp_energy = self.interp_energies[self.r]   #确定差值函数（generator）\n",
    "        self.interp_overlap = self.interp_overlaps[self.r]\n",
    "        \n",
    "        self.time = 0.0\n",
    "        self.alfa = 0.0\n",
    "        self.steps = 0\n",
    "        \n",
    "        self.protocol = []               \n",
    "        self.protocol.append(0)             #初始0时刻alpha为零\n",
    "        self.action_sequence = []\n",
    "\n",
    "        self.cum_reward = 0.0\n",
    "                \n",
    "        self.state =  [self.alfa/self.max_alfa] + [self.time/self.t_max]  #都是零，为何这样赋值？\n",
    "        \n",
    "        return np.array(self.state)       \n",
    "    \n",
    "    ## Take one step, and give reward as feedback.\n",
    "    def step(self, action): \n",
    "        \n",
    "        self.action_sequence.append(action)\n",
    "        self.time += self.alfa_dt  #真实时间\n",
    "        self.dalfa = self.action_space[action]  \n",
    "                \n",
    "        self.new_alfa = self.alfa + self.dalfa*self.alfa_dt  \n",
    "        \n",
    "        if self.new_alfa < 0.0:\n",
    "            reward = -10\n",
    "        elif self.new_alfa > self.max_alfa:\n",
    "            reward = -10\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        self.new_alfa = np.clip(self.new_alfa, 0.0, self.max_alfa)\n",
    "        \n",
    "        self.steps += 1     \n",
    "        self.protocol.append(self.new_alfa)\n",
    " \n",
    "    \n",
    "        self.alfa = self.new_alfa\n",
    "\n",
    "        next_state = [self.alfa/self.max_alfa] + [self.time/self.t_max]  #为何 \n",
    "\n",
    "        \n",
    "        if self.steps == self.N_alfa:\n",
    "            \n",
    "            N_t = 1000\n",
    "            reward += 100*self.interp_reward(N_t)*self.reward_distribution[self.r]  #reward_distribution的元素都是1，为何要有它？          \n",
    "            done = True            \n",
    "            #if  reward > self.best_reward:\n",
    "             #   self.best_protocol = self.protocol\n",
    "             #   self.best_reward = reward\n",
    "                #print('REWARD: {}'.format(reward))\n",
    "        else:\n",
    "            done = False\n",
    "         \n",
    "        \n",
    "        self.cum_reward += reward  #reward只有两种方式可以得到值，区域越出或者最后self.steps == self.N_alfa\n",
    "        \n",
    "        if done == True:\n",
    "            \n",
    "            self.scores.append(self.cum_reward)\n",
    "        \n",
    "        return np.array(next_state), reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on implementation in Keras Deep Learning Cookbook by Rajdeep Dua et al. Rajdeep Dua 的书值得一看！\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_space, action_space):\n",
    "        \n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.memory = deque(maxlen = 50000)  #用来存状态？\n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99995          #这几个epsilon值是什么？ \n",
    "        self.epsilon_min = 0.05\n",
    "        \n",
    "        self.learning_rate = 0.001        \n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential([\n",
    "            Dense(24, input_dim = self.state_space.shape[0], activation = 'relu'), #这里的self.state_space应该是keras张量\n",
    "            Dense(48, activation = 'relu'),\n",
    "            Dense(24, activation = 'relu'),\n",
    "            Dense(self.action_space.shape[0], activation = 'linear')\n",
    "        ])\n",
    "        \n",
    "        model.compile(loss='mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(len(self.action_space)-1)\n",
    "        \n",
    "        policy = self.model.predict(state)  #通过model来确定【state，action】的value, 这里predict的结果是？\n",
    "        best_action = np.argmax(policy[0])  #\n",
    "        \n",
    "        return best_action\n",
    " \n",
    "    \n",
    "    def add_to_memory(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    def fit_from_memory(self, batch_size):\n",
    "    \n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            \n",
    "            target = self.target_model.predict(state)\n",
    "            \n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "                \n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(next_state)[0])\n",
    "                target[0][action] = reward + Q_future*self.gamma\n",
    "                \n",
    "            self.model.fit(state, target, epochs = 1, verbose = 0)   \n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/vegard/Split-wavefunction/data/alfa/alfa_a_0.530.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c3efce28275a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSzilard_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstate_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maction_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a991d5e85fa2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0masymmetry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.53\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masymmetries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masymmetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0menergy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masymmetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_energies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterp_overlaps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a991d5e85fa2>\u001b[0m in \u001b[0;36minterp_generator\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mmax_lvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtab_alfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/vegard/Split-wavefunction/data/alfa/alfa_a_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"{:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0menergies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/vegard/Split-wavefunction/data/energies/Energies_a_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"{:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0moverlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/vegard/Split-wavefunction/data/overlap/overlap_a_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"{:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PythonData/lib/python3.6/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    616\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/vegard/Split-wavefunction/data/alfa/alfa_a_0.530.txt not found."
     ]
    }
   ],
   "source": [
    "env = Szilard_env()\n",
    "state_space = env.state_space\n",
    "action_space = env.action_space\n",
    "\n",
    "batch_size = 32\n",
    "n_episodes = 40000\n",
    "tau = 1e-3\n",
    "\n",
    "agent = Agent(state_space, action_space)\n",
    "agent.epsilon_decay = (0.05)**(1/n_episodes)\n",
    "agent.memory = deque(maxlen = 200000)\n",
    "\n",
    "score = [None]*n_episodes\n",
    "\n",
    "output_dir = 'model_output/Model_6/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(\"Training on {} asymmetries in the range : {}\".format(env.N_asymmetries,env.asymmetries))\n",
    "print(\"Available actions: \\n{}\".format(env.action_space))\n",
    "print(\"T_max = {}, Number of steps = {}, dt = {}\".format(env.t_max, env.N_alfa, env.alfa_dt))\n",
    "print(\"Hyperparameters: memorysize = {}, tau = {}, batch_size = {}\".format(len(agent.memory),tau, batch_size))\n",
    "print(\"Total number of experiences = {}\".format(env.N_alfa*n_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, (1,-1))\n",
    "    \n",
    "    for t in range(1000):\n",
    "        \n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "        next_state = np.reshape(next_state, (1,-1))\n",
    "        \n",
    "        agent.add_to_memory(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print('episode: {}/{}, score: {:.3}, epsilon: {:.2}'.format(e,n_episodes, env.cum_reward, agent.epsilon))\n",
    "            score[e] = env.cum_reward\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.fit_from_memory(batch_size)\n",
    "\n",
    "            target_weights = np.array(agent.target_model.get_weights())\n",
    "            current_weights = np.array(agent.model.get_weights())\n",
    "\n",
    "            agent.target_model.set_weights(current_weights*tau + target_weights*(1-tau))\n",
    "        \n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        \n",
    "        agent.model.save_weights(output_dir + \"weights_\" + \"{:04d}\".format(e) + \".hdf5\")\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = [n for n in range(len(score))]\n",
    "plt.scatter(episodes,score, s = 1)\n",
    "#plt.ylim([-1000,0])\n",
    "print(\"Training on {} asymmetries in the range : {}\".format(env.N_asymmetries,env.asymmetries))\n",
    "print(\"Available actions: \\n{}\".format(env.action_space))\n",
    "print(\"T_max = {}, Number of steps = {}, dt = {}\".format(env.t_max, env.N_alfa, env.alfa_dt))\n",
    "print(\"Hyperparameters: memorysize = {}, tau = {}, batch_size = {}\".format(len(agent.memory),tau, batch_size))\n",
    "print(\"Total number of experiences = {}\".format(env.N_alfa*n_episodes))\n",
    "print(\"Reward distribution: {}\".format(np.around(env.reward_distribution,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load earlier instances of the agent, and find its best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_agent = DQNAgent(state_size, action_size)\n",
    "loaded_agent.model.load_weights('model_output/Model_1/weights_10000.hdf5')\n",
    "\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1,state_size])\n",
    "loaded_agent.epsilon = 0.0\n",
    "\n",
    "for t in range(env.N_alfa):\n",
    "\n",
    "    action = loaded_agent.act(state)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    next_state = np.reshape(next_state, [1,state_size])\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env.protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = np.reshape(state, [1,state_size])\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "for t in range(env.N_alfa):\n",
    "\n",
    "    action = agent.act(state)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    next_state = np.reshape(next_state, [1,state_size])\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on different asymmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol = env.protocol\n",
    "\n",
    "N = env.N_asymmetries\n",
    "asymmetries = env.asymmetries\n",
    "\n",
    "leakage = [0 for n in range (N)]\n",
    "asymmetry = [0 for n in range (N)]\n",
    "punishment = [0 for n in range (N)]\n",
    "C_array = [0 for n in range (N)]\n",
    "\n",
    "for n in range (N):\n",
    "    print(asymmetries[n])\n",
    "    interp_energy, interp_overlap = interp_generator(asymmetries[n])\n",
    "    \n",
    "    C_array[n],leakage[n], punishment[n] = interp_reward(0.05,1000,protocol)\n",
    "  \n",
    "plt.plot(asymmetries,punishment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(asymmetries,leakage)\n",
    "plt.plot(asymmetries,punishment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(N):\n",
    "    \n",
    "    plt.scatter(asymmetries[n],abs(C_array[n][0])**2, color = 'red')\n",
    "    plt.scatter(asymmetries[n],abs(C_array[n][1])**2, color = 'blue')\n",
    "    plt.scatter(asymmetries[n],sum(abs(C_array[n][2:])**2), color = 'green')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_reward(sigma, N_t, protocol):\n",
    "\n",
    "        time = np.array([ n*env.alfa_dt for n in range (env.N_alfa+1) ])\n",
    "        alfa_interp = interp1d( time,protocol, kind = 'cubic')\n",
    "\n",
    "        time = np.linspace(0,env.t_max,N_t)\n",
    "        alfa = alfa_interp(time)\n",
    "        alfa = np.clip(alfa,0,5000)\n",
    "        dt = time[1]-time[0]\n",
    "        max_lvl = env.max_lvl\n",
    " \n",
    "        E = []\n",
    "        for t in range(N_t):\n",
    "            E.append([interp_energy[n](alfa[t]) for n in range(max_lvl)])\n",
    "        E = np.array(E)\n",
    "        \n",
    "        phi = []\n",
    "        for t in range(N_t):\n",
    "                integral = [np.trapz(x=time[0:t+1], y= E[0:t+1,n]) for n in range(max_lvl)]\n",
    "                phi.append(integral)  \n",
    "        phi = np.array(phi)   \n",
    "\n",
    "        def interaction_int(n, alfa_in, f = []):\n",
    "            test = sum([interp_overlap[n][m](abs(alfa_in))*np.exp(1j*(phi[t,n] - phi[t,m]) )*f[m] for m in range(max_lvl) if m != n ])*dalfa\n",
    "            return test\n",
    "\n",
    "        C = np.array([ 0 for n in range(max_lvl) ])\n",
    "        C[0] = 1\n",
    "        C_new = np.array( [ 0 for n in range(max_lvl) ])\n",
    "        C_t = []\n",
    "        C_t.append(C)\n",
    "\n",
    "        for t in range(N_t -1):\n",
    "            \n",
    "            dalfa = (alfa[t+1] - alfa[t])/dt\n",
    "\n",
    "            k1 = np.array([dt*interaction_int(n, alfa[t], C ) for n in range(max_lvl)])\n",
    "            k2 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt/2, C + k1/2 ) for n in range(max_lvl)])\n",
    "            k3 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt/2, C + k2/2 ) for n in range(max_lvl)])\n",
    "            k4 = np.array([dt*interaction_int(n, alfa[t]+ dalfa*dt, C + k3) for n in range(max_lvl)])\n",
    "\n",
    "            C_new = C + (k1 + 2*k2 + 2*k3 + k4)/6\n",
    "\n",
    "            C = C_new\n",
    "\n",
    "            C_t.append(C)\n",
    "\n",
    "        #C_t = np.array(C_t)\n",
    "\n",
    "        c1 = abs(C[0])**2\n",
    "        c2 = abs(C[1])**2\n",
    "\n",
    "        leakage = sum( abs(C[n])**2 for n in range(2,max_lvl) )\n",
    "        reward = np.exp( -((c1-0.5)**2 + (c2-0.5)**2)/(sigma))\n",
    "        \n",
    "        #print(\"Normalization_int: {}\" .format(sum(abs(C)**2)))\n",
    "        #print(abs(C)**2)\n",
    "        #plt.plot(alfa(time),abs(np.array(C_t))**2)\n",
    "        #plt.plot(time,abs(alfa(time)))\n",
    "        plt.plot(time,alfa)\n",
    "        return C, leakage, reward   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_generator(a):\n",
    "    \n",
    "    max_lvl = 4 \n",
    "\n",
    "    tab_alfa = np.loadtxt('/home/vegard/Split-wavefunction/data/alfa/alfa_a_' + \"{:.3f}\".format(a) + '.txt', delimiter = ',')        \n",
    "    energies = np.loadtxt('/home/vegard/Split-wavefunction/data/energies/Energies_a_' + \"{:.3f}\".format(a) + '.txt', delimiter = ',')\n",
    "    overlap = np.loadtxt('/home/vegard/Split-wavefunction/data/overlap/overlap_a_' + \"{:.3f}\".format(a)+ '.txt', delimiter = ',')\n",
    "\n",
    "    overlap = np.reshape(overlap, [max_lvl,max_lvl,5001], order = 'F')\n",
    "\n",
    "\n",
    "    interp_energy = [None]*max_lvl\n",
    "\n",
    "    for n in range(max_lvl):\n",
    "        f = interp1d(tab_alfa,energies[:,n], kind = 'cubic')\n",
    "        interp_energy[n] = f\n",
    "\n",
    "    interp_overlap = [[0] * max_lvl for i in range(max_lvl)]\n",
    "\n",
    "    for n in range(max_lvl):\n",
    "        for m in range(max_lvl):\n",
    "            interp_overlap[n][m] = interp1d(tab_alfa,overlap[n,m,:], kind = 'cubic')\n",
    "    \n",
    "    return interp_energy, interp_overlap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
